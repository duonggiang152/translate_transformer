{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from positional_encoding_layer import PositionalEncodingLayer\n",
    "from transformer_model import TransformerBuilder\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentences(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        sentences = file.readlines()\n",
    "    sentences = [sentence.strip() for sentence in sentences]\n",
    "    return sentences\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"/home/giang/data/eng_vi/train.csv\") # from this dataset https://huggingface.co/datasets/ncduy/mt-en-vi\n",
    "\n",
    "# Extract English and Vietnamese columns\n",
    "english_sentences_csv = df['en'].tolist()\n",
    "vietnamese_sentences_cvs = df['vi'].tolist()\n",
    "\n",
    "# Print the pairs\n",
    "pairs_csv = list(zip(english_sentences_csv, vietnamese_sentences_cvs))\n",
    "\n",
    "english_file_path = \"/home/giang/data/eng_vi/train.en.txt\"  # from this dataset https://www.kaggle.com/datasets/tuannguyenvananh/iwslt15-englishvietnamese\n",
    "english_sentences = load_sentences(english_file_path)\n",
    "\n",
    "\n",
    "vietnamese_file_path = \"/home/giang/data/eng_vi/train.vi.txt\"  \n",
    "vietnamese_sentences = load_sentences(vietnamese_file_path) # from this dataset https://www.kaggle.com/datasets/tuannguyenvananh/iwslt15-englishvietnamese\n",
    "\n",
    "\n",
    "pairs_txt = list(zip(english_sentences, vietnamese_sentences))\n",
    "pairs =  pairs_csv + pairs_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = tf.data.Dataset.from_tensor_slices(pairs[:10]).batch(2)\n",
    "for i in dataset_test:\n",
    "    print(i)\n",
    "    print(\"/////////////////\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(pairs)\n",
    "sentences_en, sentences_vn = zip(*pairs)\n",
    "print(len(pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print(sentences_en[i], \"=>\", sentences_vn[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_output(sentences_en, sentences_vn):\n",
    "\n",
    "    sentence_eng_dataset = tf.data.Dataset.from_tensor_slices(list(sentences_en))\n",
    "    sentence_vn_dataset = tf.data.Dataset.from_tensor_slices(list(sentences_vn))\n",
    "\n",
    "    sentence_eng_dataset = sentence_eng_dataset.batch(100000)\n",
    "    sentence_vn_dataset = sentence_vn_dataset.batch(100000)\n",
    "    sentence_vn_dataset = sentence_vn_dataset.map(lambda batch: \"startofseq \" + batch+ \" endofseq\")\n",
    "\n",
    "\n",
    "    return (sentence_eng_dataset, sentence_vn_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sentence_eng_dataset, sentence_vn_dataset) = generate_dataset_output(sentences_en, sentences_vn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "max_length = 500\n",
    "\n",
    "text_vec_layer_en = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=vocab_size,  \n",
    "    output_sequence_length=max_length,\n",
    ")\n",
    "\n",
    "text_vec_layer_vn = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=vocab_size,  \n",
    "    output_sequence_length=max_length \n",
    ")\n",
    "\n",
    "text_vec_layer_en.adapt(sentences_en)\n",
    "\n",
    "text_vec_layer_vn.adapt([f\"startofseq {s} endofseq\" for s in sentences_vn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = text_vec_layer_vn.get_vocabulary()\n",
    "with open(\"vectorizer_vocab.txt\", \"w\") as f:\n",
    "    for word in vocab:\n",
    "        f.write(word + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_model = tf.keras.Sequential([text_vec_layer_vn])\n",
    "vectorizer_model.save(\"text_vectorizer.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_vec_layer_en.get_vocabulary()[:20])\n",
    "print(text_vec_layer_vn.get_vocabulary()[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_function(pair):\n",
    "        global text_vec_layer_vn\n",
    "        output_vectorize = text_vec_layer_vn(pair[1] + \" endofseq\")\n",
    "        return ((pair[0], \"startofseq \" + pair[1]), output_vectorize)\n",
    "def generate_valid_train_dataset():\n",
    "    train_data_size = int((len(pairs) * 0.9))\n",
    "    train_data = pairs[:train_data_size]\n",
    "    valid_data = pairs[train_data_size:]\n",
    "    train_data_dataset = tf.data.Dataset.from_tensor_slices(train_data).map(transform_function).batch(70)\n",
    "    valid_data_dataset = tf.data.Dataset.from_tensor_slices(valid_data).map(transform_function).batch(70)\n",
    "    return (train_data_dataset, valid_data_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data_dataset, valid_data_dataset) = generate_valid_train_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_data_dataset:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
    "decoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 128\n",
    "encoder_input_ids = text_vec_layer_en(encoder_inputs)\n",
    "decoder_input_ids = text_vec_layer_vn(decoder_inputs)\n",
    "encoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size, mask_zero=True)\n",
    "decoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size, mask_zero=True)\n",
    "encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
    "decoder_embeddings = decoder_embedding_layer(decoder_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_embed_layer = PositionalEncodingLayer(max_length, embed_size)\n",
    "encoder_in = pos_embed_layer(encoder_embeddings)\n",
    "decoder_in = pos_embed_layer(decoder_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_builder = TransformerBuilder()\n",
    "transformer_output = transformer_builder.build(encoder_in=encoder_in, decoder_in=decoder_in)\n",
    "Y_proba = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")(transformer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],outputs=[Y_proba])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"checkpoints/march_19_test2/model_{epoch:02d}_{val_loss:.2f}.weights.h5\",  # Use .h5\n",
    "    save_weights_only=True,\n",
    "    save_best_only=False,\n",
    "    mode=\"min\",\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_data_dataset,  # Training dataset\n",
    "    epochs=2,      # Number of epochs\n",
    "    validation_data=valid_data_dataset,  # Validation dataset\n",
    "    callbacks=[checkpoint_callback]\n",
    "\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model('saved_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence_en):\n",
    "    translation = \"\"\n",
    "    for word_idx in range(50):\n",
    "        X = np.array([sentence_en]) # encoder input\n",
    "        X_dec = np.array([\"startofseq \" + translation]) # decoder input\n",
    "        y_proba = loaded_model((X, X_dec))[0, word_idx] # last token's probas\n",
    "        predicted_word_id = np.argmax(y_proba)\n",
    "        predicted_word = text_vec_layer_vn.get_vocabulary()[predicted_word_id]\n",
    "        if predicted_word == \"endofseq\":\n",
    "            break\n",
    "        translation += \" \" + predicted_word\n",
    "    return translation.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def translate_with_beam_search(sentence_en, beam_width=3):\n",
    "    beam = [(\"startofseq \", 0.0)]  \n",
    "    max_length = 20  \n",
    "    for word_idx in range(max_length):\n",
    "        new_beam = []\n",
    "        for translation, score in beam:\n",
    "            if translation.endswith(\"endofseq\"):\n",
    "                new_beam.append((translation, score))\n",
    "                continue\n",
    "            X = np.array([sentence_en])  \n",
    "            X_dec = np.array([translation]) \n",
    "\n",
    "            y_proba = model((X, X_dec))[0, word_idx]  # Probability distribution\n",
    "            \n",
    "            \n",
    "            y_proba_tensor = tf.convert_to_tensor(y_proba)  \n",
    "            top_k_values, top_k_indices = tf.math.top_k(y_proba_tensor, k=beam_width)\n",
    "\n",
    "            top_k_indices = top_k_indices.numpy()\n",
    "            top_k_values = top_k_values.numpy()\n",
    "\n",
    "            for word_id, word_score in zip(top_k_indices, top_k_values):\n",
    "                predicted_word = text_vec_layer_vn.get_vocabulary()[word_id]\n",
    "                new_translation = f\"{translation} {predicted_word}\" if translation != \"startofseq\" else predicted_word\n",
    "                new_score = score + word_score \n",
    "                new_beam.append((new_translation, new_score))\n",
    "\n",
    "\n",
    "        beam = sorted(new_beam, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "    best_translation = beam[0][0]\n",
    "    return best_translation.replace(\"startofseq\", \"\").replace(\"endofseq\", \"\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(translate(\"I need a fork\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
